name: About Blank Scraper

on:
  # Run daily at midnight UTC (adjust timezone as needed)
  schedule:
    - cron: '0 0 * * *'  # Daily at midnight UTC
  # Allow manual triggering
  workflow_dispatch:
    inputs:
      mode:
        description: 'Run mode'
        required: false
        default: 'full'
        type: choice
        options:
          - full
          - test

env:
  SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
  SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y protobuf-compiler

    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install sentencepiece
        pip install -r requirements.txt

    - name: Create config file
      run: |
        cat > config.py << EOF
        import os
        from dotenv import load_dotenv

        load_dotenv()

        SUPABASE_URL = "${{ secrets.SUPABASE_URL }}"
        SUPABASE_ANON_KEY = "${{ secrets.SUPABASE_ANON_KEY }}"

        # Scraper Configuration
        BASE_URL = "https://about---blank.com"
        SHOP_ALL_URL = f"{BASE_URL}/collections/shop-all"
        HEADERS = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }

        # Data Mapping Configuration
        BRAND = "About Blank"
        SOURCE = "scraper"
        COUNTRY = "US"
        CURRENCY = "USD"

        # Category mapping
        CATEGORY_MAPPING = {
            'clothes': ['t-shirts', 'hoodies & sweats', 'knitwear', 'outerwear', 'shirts', 'vests'],
            'accessories': ['accessories', 'headwear'],
            'footwear': []
        }

        # Rate limiting
        REQUESTS_PER_SECOND = 2
        MAX_CONCURRENT_REQUESTS = 5

        # Image processing
        EMBEDDING_MODEL = "google/siglip-base-patch16-384"
        EMBEDDING_DIM = 768
        EOF

    - name: Run scraper
      run: |
        if [ "${{ github.event.inputs.mode }}" = "test" ]; then
          echo "Running in test mode..."
          python main_test.py
        else
          echo "Running full scraper..."
          python main.py
        fi
      env:
        SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
        SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}

    - name: Upload logs
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: scraper-logs-${{ github.run_number }}
        path: |
          *.log
          logs/