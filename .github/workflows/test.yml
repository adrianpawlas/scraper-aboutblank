name: Test Scraper

on:
  push:
    branches: [ master, main ]
  pull_request:
    branches: [ master, main ]
  # Allow manual testing
  workflow_dispatch:

env:
  SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
  SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}

jobs:
  test:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y protobuf-compiler

    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install sentencepiece
        pip install -r requirements.txt

    - name: Create config file
      run: |
        cat > config.py << EOF
        import os
        from dotenv import load_dotenv

        load_dotenv()

        SUPABASE_URL = "${{ secrets.SUPABASE_URL }}"
        SUPABASE_ANON_KEY = "${{ secrets.SUPABASE_ANON_KEY }}"

        # Scraper Configuration
        BASE_URL = "https://about---blank.com"
        SHOP_ALL_URL = f"{BASE_URL}/collections/shop-all"
        HEADERS = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }

        # Data Mapping Configuration
        BRAND = "About Blank"
        SOURCE = "scraper"
        COUNTRY = "US"
        CURRENCY = "USD"

        # Category mapping
        CATEGORY_MAPPING = {
            'clothes': ['t-shirts', 'hoodies & sweats', 'knitwear', 'outerwear', 'shirts', 'vests'],
            'accessories': ['accessories', 'headwear'],
            'footwear': []
        }

        # Rate limiting
        REQUESTS_PER_SECOND = 2
        MAX_CONCURRENT_REQUESTS = 5

        # Image processing
        EMBEDDING_MODEL = "google/siglip-base-patch16-384"
        EMBEDDING_DIM = 768
        EOF

    - name: Run basic tests
      run: |
        echo "Testing imports..."
        python -c "from scraper import AboutBlankScraper; print('✓ Scraper import OK')"
        python -c "from embedding import get_embedder; print('✓ Embedding import OK')"
        python -c "from database import get_db_manager; print('✓ Database import OK')"

    - name: Run test scraper
      run: |
        echo "Running test scraper..."
        python main_test.py
      env:
        SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
        SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}

    - name: Upload test logs
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: test-logs-${{ github.run_number }}
        path: |
          *.log
          logs/